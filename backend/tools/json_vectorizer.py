"""backend.tools.json_vectorizer
---------------------------------
å°†ç»“æ„åŒ–ç—…ä¾‹ JSON è½¬æ¢ä¸ºå‘é‡å¹¶ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ vector_db/ã€‚
è¿è¡Œç¤ºä¾‹ï¼š
   python -m backend.tools.json_vectorizer backend/json_db --append
"""

import json
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import os, pickle
from typing import List, Tuple
from pathlib import Path
from backend.config import EMBED_MODEL_DIR

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).resolve().parents[2]

# æ–°å¢: ç»Ÿä¸€è·å–ç–¾ç—…åˆ—è¡¨çš„å·¥å…·å‡½æ•°ï¼Œå…¼å®¹ä¸åŒ JSON ç»“æ„
def _get_diseases_from_data(data: dict) -> list:
    """è¿”å›ä¸€ä¸ªç–¾ç—…å­—å…¸åˆ—è¡¨ï¼Œå…¼å®¹ä»¥ä¸‹ä¸¤ç§ç»“æ„:  
    1. data['possible_diseases'] -> List[dict]  
    2. data['disease'] -> dict
    """
    if 'possible_diseases' in data and isinstance(data['possible_diseases'], list):
        return data['possible_diseases']
    elif 'disease' in data and isinstance(data['disease'], dict):
        return [data['disease']]
    # é»˜è®¤è¿”å›ç©ºåˆ—è¡¨ï¼Œè°ƒç”¨æ–¹è‡ªè¡Œå¤„ç†
    return []

class JSONVectorizer:
    def __init__(self):
        # å¦‚æœæœ¬åœ°æ¨¡å‹ç›®å½•å­˜åœ¨åˆ™ä¼˜å…ˆä½¿ç”¨ï¼Œå¦åˆ™å›é€€åœ¨çº¿æ¨¡å‹å
        if EMBED_MODEL_DIR.exists():
            # snapshots ç»“æ„å¤„ç†
            snapshots = list((EMBED_MODEL_DIR / "snapshots").glob("*"))
            model_path = snapshots[0] if snapshots else EMBED_MODEL_DIR
            print(f"âœ… ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹: {model_path}")
            self.model = SentenceTransformer(str(model_path))
        else:
            print("âš ï¸ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†åœ¨çº¿åŠ è½½ 'sentence-transformers/all-MiniLM-L6-v2'")
            self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    
    def extract_features(self, data):
        """æå–JSONç‰¹å¾ (å…¼å®¹ possible_diseases ä¸ disease ä¸¤ç§ç»“æ„)"""
        features = {}

        # ç—‡çŠ¶ç‰¹å¾
        features['symptom'] = f"{data.get('symptom', '')} {data.get('symptom_category', '')} {' '.join(data.get('severity_indicators', []))}"

        # ç–¾ç—…ç‰¹å¾
        disease_texts = []
        diseases = _get_diseases_from_data(data)
        for disease in diseases:
            disease_name = disease.get('disease') or disease.get('name', '')
            probability = disease.get('probability', '')
            urgency = disease.get('urgency_level', '')
            key_features = disease.get('key_features', [])
            text = f"{disease_name} æ¦‚ç‡:{probability} ç´§æ€¥:{urgency} {' '.join(key_features)}"
            disease_texts.append(text)
        features['diseases'] = disease_texts

        # æ£€æŸ¥ç‰¹å¾
        test_texts = []
        for disease in diseases:
            for test in disease.get('recommended_tests', []):
                test_name = test.get('test') or test.get('test_name', '')
                purpose = test.get('purpose', '')
                positive_indicators = test.get('positive_indicators', [])
                text = f"{test_name} {purpose} {' '.join(positive_indicators)}"
                test_texts.append(text)
        features['tests'] = test_texts

        return features
    
    def vectorize(self, data):
        """å‘é‡åŒ–JSONæ•°æ® - æ¯ä¸ªå­—æ®µå•ç‹¬å‘é‡åŒ–"""
        features = self.extract_features(data)
        
        # 1. ç—‡çŠ¶å‘é‡ï¼ˆä¸»è¦æ£€ç´¢ç›®æ ‡ï¼‰
        symptom_vector = self.model.encode(features['symptom'])
        symptom_vector = symptom_vector / np.linalg.norm(symptom_vector)  # L2å½’ä¸€åŒ–
        
        # 2. ç–¾ç—…å‘é‡ï¼ˆæ¯ä¸ªç–¾ç—…ä¸€ä¸ªå‘é‡ï¼‰
        disease_vectors = []
        for text in features['diseases']:
            vec = self.model.encode(text)
            vec = vec / np.linalg.norm(vec)
            disease_vectors.append(vec)
        
        # 3. æ£€æŸ¥å‘é‡ï¼ˆæ¯ä¸ªæ£€æŸ¥ä¸€ä¸ªå‘é‡ï¼‰
        test_vectors = []
        for text in features['tests']:
            vec = self.model.encode(text)
            vec = vec / np.linalg.norm(vec)
            test_vectors.append(vec)
        
        return {
            'symptom_vector': symptom_vector,
            'disease_vectors': disease_vectors,
            'test_vectors': test_vectors
        }

def build_and_save_index(json_files: List[str], output_dir: str | None = None,
                       append: bool = False):
    """
    æ ¹æ®ç»™å®šçš„ JSON æ–‡ä»¶åˆ—è¡¨æ„å»º FAISS å‘é‡ç´¢å¼•ï¼Œå¹¶å°†ç´¢å¼•ä¸å…ƒæ•°æ®ä¿å­˜åˆ°ç£ç›˜ã€‚
    æ¯ä¸ª JSON æ–‡ä»¶ä¼šç”Ÿæˆå¤šä¸ªå‘é‡æ¡ç›®ï¼š
    - 1ä¸ªç—‡çŠ¶å‘é‡
    - Nä¸ªç–¾ç—…å‘é‡ï¼ˆæ¯ä¸ªç–¾ç—…ä¸€ä¸ªï¼‰
    - Mä¸ªæ£€æŸ¥å‘é‡ï¼ˆæ¯ä¸ªæ£€æŸ¥ä¸€ä¸ªï¼‰
    """
    if output_dir is None:
        output_dir = str(PROJECT_ROOT / "vector_db")
    os.makedirs(output_dir, exist_ok=True)
    vectorizer = JSONVectorizer()

    vectors: List[np.ndarray] = []  # æ–°å¢å‘é‡
    metadata: List[dict] = []      # æ–°å¢å…ƒæ•°æ®

    for path in json_files:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        vecs = vectorizer.vectorize(data)
        
        # 1. æ·»åŠ ç—‡çŠ¶å‘é‡
        vectors.append(vecs['symptom_vector'])
        metadata.append({
            'file_path': path,
            'type': 'symptom_case',
            'content': data.get('symptom', ''),
            'category': data.get('symptom_category', ''),
            'full_data': data
        })
        
        # 2. æ·»åŠ ç–¾ç—…å‘é‡
        diseases = _get_diseases_from_data(data)
        for i, disease_vec in enumerate(vecs['disease_vectors']):
            vectors.append(disease_vec)
            disease_name = diseases[i].get('disease') or diseases[i].get('name', '')
            metadata.append({
                'file_path': path,
                'type': 'disease',
                'content': disease_name,
                'category': data.get('symptom_category', ''),
                'original_data': data
            })
        
        # 3. æ·»åŠ æ£€æŸ¥å‘é‡
        test_idx = 0
        for disease in diseases:
            for test in disease.get('recommended_tests', []):
                if test_idx < len(vecs['test_vectors']):
                    vectors.append(vecs['test_vectors'][test_idx])
                    test_name = test.get('test') or test.get('test_name', '')
                    metadata.append({
                        'file_path': path,
                        'type': 'test',
                        'content': test_name,
                        'category': data.get('symptom_category', ''),
                        'original_data': data
                    })
                    test_idx += 1
                else:
                    print(f"âš ï¸ æ£€æŸ¥å‘é‡æ•°é‡ä¸è¶³: éœ€è¦ {test_idx + 1} ä¸ªï¼Œä½†åªæœ‰ {len(vecs['test_vectors'])} ä¸ª")

    if not vectors:
        raise ValueError("æœªæä¾›ä»»ä½• JSON æ–‡ä»¶ï¼Œæ— æ³•æ„å»ºç´¢å¼•ï¼")

    dim = len(vectors[0])

    index_path = os.path.join(output_dir, 'medical.index')
    meta_path = os.path.join(output_dir, 'metadata.pkl')

    if append and os.path.exists(index_path) and os.path.exists(meta_path):
        # è¿½åŠ æ¨¡å¼ï¼šåŠ è½½ç°æœ‰ç´¢å¼•ä¸å…ƒæ•°æ®
        index = faiss.read_index(index_path)
        if index.d != dim:
            raise ValueError(f"å‘é‡ç»´åº¦ä¸åŒ¹é…: ç°æœ‰ {index.d}, æ–°æ•°æ® {dim}")
        with open(meta_path, 'rb') as f:
            old_metadata: List[dict] = pickle.load(f)
        index.add(np.array(vectors).astype('float32'))
        metadata = old_metadata + metadata
    else:
        # å…¨é‡é‡å»º
        index = faiss.IndexFlatIP(dim)
        index.add(np.array(vectors).astype('float32'))

    faiss.write_index(index, index_path)
    with open(meta_path, 'wb') as f:
        pickle.dump(metadata, f)

    print(f"âœ… å‘é‡ç´¢å¼•å·²ä¿å­˜åˆ° {index_path}")
    print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜åˆ° {meta_path}")
    print(f"ğŸ“Š æ€»å…± {len(vectors)} ä¸ªå‘é‡æ¡ç›®")
    print(f"ğŸ“ æ¥è‡ª {len(json_files)} ä¸ª JSON æ–‡ä»¶")


# ---------------- CLI ----------------

def _cli() -> None:
    """å‘½ä»¤è¡Œå…¥å£: æ”¯æŒé€šé…ç¬¦ã€ç›®å½•å‚æ•°ï¼Œé•¿æœŸå¯ç”¨ã€‚"""
    import argparse, glob, os
    from pathlib import Path

    parser = argparse.ArgumentParser(description="æ‰¹é‡å‘é‡åŒ– JSON å¹¶ç”Ÿæˆç´¢å¼•")
    parser.add_argument(
        "paths",
        nargs="+",
        help="JSON æ–‡ä»¶æˆ–ç›®å½•, æ”¯æŒé€šé…ç¬¦ (å¦‚ data/*.json)",
    )
    parser.add_argument(
        "-o",
        "--output",
        default=None,
        help="ç´¢å¼•è¾“å‡ºç›®å½•, é»˜è®¤ vector_db/",
    )
    parser.add_argument(
        "--append",
        action="store_true",
        help="è¿½åŠ æ¨¡å¼: ä¿ç•™å·²æœ‰å‘é‡åº“å¹¶æ’å…¥æ–°æ•°æ®",
    )
    args = parser.parse_args()

    # æ”¶é›† JSON æ–‡ä»¶
    json_files: list[str] = []
    for p in args.paths:
        if os.path.isdir(p):
            json_files.extend(glob.glob(os.path.join(p, "*.json")))
        else:
            json_files.extend(glob.glob(p))

    json_files = [str(Path(f)) for f in json_files if f.lower().endswith(".json")]
    if not json_files:
        parser.error("æœªæ‰¾åˆ°ä»»ä½• JSON æ–‡ä»¶ï¼")

    build_and_save_index(json_files, output_dir=args.output, append=args.append)


if __name__ == "__main__":
    _cli() 